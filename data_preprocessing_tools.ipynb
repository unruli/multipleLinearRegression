{
 "cells": [],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "{\n",
     " \"cells\": [\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"This notebook demonstrates how to leverage transfer learning to use your own image dataset to build and train an image classification model using MXNet and Amazon SageMaker.\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"We use, as an example, the creation of a trash classification model which, given some image, classifies it into one of three classes: compost, landfill, recycle. This is based on the [Show Before You Throw](https://www.youtube.com/watch?v=Ut1VGG6TOOw) project from an AWS DeepLens hackathon and the [Smart Recycle Arm](https://www.youtube.com/watch?v=QF0QjRjBwFs) project presented at the AWS Public Sector Summit 2019\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"--- \\n\",\n",
     "    \"\\n\",\n",
     "    \"1. [Prerequisites](#Prerequisites)\\n\",\n",
     "    \"1. [Download Data](#Download-data)\\n\",\n",
     "    \"1. [Fine-tuning the Image Classification Model](#Fine-tuning-the-Image-classification-model)\\n\",\n",
     "    \"1. [Start the Training](#Start-the-training)\\n\",\n",
     "    \"1. [Test your Model](#Inference)\\n\",\n",
     "    \"1. [Deploy your Model to AWS DeepLens](#Deploy)\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## Prequisites\\n\",\n",
     "    \"\\n\",\n",
     "    \"- Amazon Sagemaker notebook should have internet access to download images needed for testing this notebook. This is turned ON by default. To explore aoptions review this link : [Sagemaker routing options](https://aws.amazon.com/blogs/machine-learning/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options/)\\n\",\n",
     "    \"- The IAM role assigned to this notebook should have permissions to create a bucket (if it does not exist)\\n\",\n",
     "    \"  - [IAM role for Amazon Sagemaker](https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role-sagemaker-notebook.html)\\n\",\n",
     "    \"  - [S3 create bucket permissions](https://docs.aws.amazon.com/AmazonS3/latest/dev/using-with-s3-actions.html#using-with-s3-actions-related-to-buckets)\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"### Permissions and environment variables\\n\",\n",
     "    \"\\n\",\n",
     "    \"Here we set up the linkage and authentication to AWS services. There are 2 parts to this:\\n\",\n",
     "    \"\\n\",\n",
     "    \"* The roles used to give learning and hosting access to your data. This will automatically be obtained from the role used to start the notebook\\n\",\n",
     "    \"* The Amazon sagemaker image classification docker image which need not be changed\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"import os\\n\",\n",
     "    \"import urllib.request\\n\",\n",
     "    \"import boto3, botocore\\n\",\n",
     "    \"\\n\",\n",
     "    \"\\n\",\n",
     "    \"import sagemaker\\n\",\n",
     "    \"from sagemaker import get_execution_role\\n\",\n",
     "    \"\\n\",\n",
     "    \"import mxnet as mx\\n\",\n",
     "    \"mxnet_path = mx.__file__[ : mx.__file__.rfind('/')]\\n\",\n",
     "    \"print(mxnet_path)\\n\",\n",
     "    \"\\n\",\n",
     "    \"role = get_execution_role()\\n\",\n",
     "    \"print(role)\\n\",\n",
     "    \"\\n\",\n",
     "    \"sess = sagemaker.Session()\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## Amazon S3 bucket info\\n\",\n",
     "    \"Enter your Amazon S3 Bucket name where your data will be stored, make sure that your SageMaker notebook has access to this S3 Bucket by granting `S3FullAccess` in the SageMaker role attached to this instance. See [here](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html) for more info.\\n\",\n",
     "    \"\\n\",\n",
     "    \"DeepLens-compatible buckets must start with `deeplens`\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"BUCKET = 'deeplens-<Your-Test-Bucket>'\\n\",\n",
     "    \"PREFIX = 'deeplens-trash-test'\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"from sagemaker.amazon.amazon_estimator import get_image_uri\\n\",\n",
     "    \"training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\\\"latest\\\")\\n\",\n",
     "    \"print (training_image)\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"We are going to check if we have the right bucket and if we have the right permissions.\\n\",\n",
     "    \"\\n\",\n",
     "    \"Please make sure that the result from this cell is \\\"Bucket access is Ok\\\"\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"test_data = 'TestData'\\n\",\n",
     "    \"s3 = boto3.resource('s3')\\n\",\n",
     "    \"object = s3.Object(BUCKET, PREFIX+\\\"/test.txt\\\")\\n\",\n",
     "    \"try:\\n\",\n",
     "    \"    object.put(Body=test_data)\\n\",\n",
     "    \"except botocore.exceptions.ClientError as e:\\n\",\n",
     "    \"    if e.response['Error']['Code'] == \\\"AccessDenied\\\":\\n\",\n",
     "    \"        #cannot write on the bucket\\n\",\n",
     "    \"        print(\\\"Bucket \\\"+BUCKET+\\\"is not writeable, make sure you have the right permissions\\\")\\n\",\n",
     "    \"    else:\\n\",\n",
     "    \"        if e.response['Error']['Code'] == \\\"NoSuchBucket\\\":\\n\",\n",
     "    \"            #Bucket does not exist\\n\",\n",
     "    \"            print(\\\"Bucket\\\"+BUCKET+\\\" does not exist\\\")\\n\",\n",
     "    \"        else:\\n\",\n",
     "    \"            raise\\n\",\n",
     "    \"else:\\n\",\n",
     "    \"    print(\\\"Bucket access is Ok\\\")\\n\",\n",
     "    \"    object.delete(BUCKET, PREFIX+\\\"/test.txt\\\")\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"# Prepare data\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"It is assumed that your custom dataset's images are present in an S3 bucket and that different classes are separated by named folders, as shown in the following directory structure:\\n\",\n",
     "    \"```\\n\",\n",
     "    \"|-deeplens-bucket\\n\",\n",
     "    \"   |-deeplens-trash\\n\",\n",
     "    \"\\n\",\n",
     "    \"    |-images\\n\",\n",
     "    \"    \\n\",\n",
     "    \"        |-Compost \\n\",\n",
     "    \"    \\n\",\n",
     "    \"        |-Landfill\\n\",\n",
     "    \"    \\n\",\n",
     "    \"        |-Recycle\\n\",\n",
     "    \" ```\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"Since we are providing the data for you in this example, first we'll download the example data, unzip it and upload it to your bucket.\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"!wget https://deeplens-public.s3.amazonaws.com/samples/deeplens-trash/trash-images.zip\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"!rm -rf data/ && mkdir -p data\\n\",\n",
     "    \"!mkdir -p data/images\\n\",\n",
     "    \"!unzip -qq trash-images.zip -d data/images\\n\",\n",
     "    \"!rm trash-images.zip\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"import matplotlib.pyplot as plt\\n\",\n",
     "    \"%matplotlib inline\\n\",\n",
     "    \"\\n\",\n",
     "    \"def show_images(item_name, images_to_show=-1):\\n\",\n",
     "    \"    _im_list = !ls $IMAGES_DIR/$item_name\\n\",\n",
     "    \"\\n\",\n",
     "    \"    NUM_COLS = 3\\n\",\n",
     "    \"    if images_to_show == -1:\\n\",\n",
     "    \"        IM_COUNT = len(_im_list)\\n\",\n",
     "    \"    else:\\n\",\n",
     "    \"        IM_COUNT = images_to_show\\n\",\n",
     "    \"    \\n\",\n",
     "    \"    print('Displaying images category ' + item_name + ' count: ' + str(IM_COUNT) + ' images.')\\n\",\n",
     "    \"    \\n\",\n",
     "    \"    NUM_ROWS = int(IM_COUNT / NUM_COLS)\\n\",\n",
     "    \"    if ((IM_COUNT % NUM_COLS) > 0):\\n\",\n",
     "    \"        NUM_ROWS += 1\\n\",\n",
     "    \"\\n\",\n",
     "    \"    fig, axarr = plt.subplots(NUM_ROWS, NUM_COLS)\\n\",\n",
     "    \"    fig.set_size_inches(10.0, 10.0, forward=True)\\n\",\n",
     "    \"\\n\",\n",
     "    \"    curr_row = 0\\n\",\n",
     "    \"    for curr_img in range(IM_COUNT):\\n\",\n",
     "    \"        # fetch the url as a file type object, then read the image\\n\",\n",
     "    \"        f = IMAGES_DIR + item_name + '/' + _im_list[curr_img]\\n\",\n",
     "    \"        a = plt.imread(f)\\n\",\n",
     "    \"\\n\",\n",
     "    \"        # find the column by taking the current index modulo 3\\n\",\n",
     "    \"        col = curr_img % NUM_ROWS\\n\",\n",
     "    \"        # plot on relevant subplot\\n\",\n",
     "    \"        if NUM_ROWS == 1:\\n\",\n",
     "    \"            axarr[curr_row].imshow(a)\\n\",\n",
     "    \"        else:\\n\",\n",
     "    \"            axarr[col, curr_row].imshow(a)\\n\",\n",
     "    \"        if col == (NUM_ROWS - 1):\\n\",\n",
     "    \"            # we have finished the current row, so increment row counter\\n\",\n",
     "    \"            curr_row += 1\\n\",\n",
     "    \"\\n\",\n",
     "    \"    fig.tight_layout()       \\n\",\n",
     "    \"    plt.show()\\n\",\n",
     "    \"        \\n\",\n",
     "    \"    # Clean up\\n\",\n",
     "    \"    plt.clf()\\n\",\n",
     "    \"    plt.cla()\\n\",\n",
     "    \"    plt.close()\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"IMAGES_DIR = 'data/images/'\\n\",\n",
     "    \"show_images(\\\"Compost\\\", images_to_show=3)\\n\",\n",
     "    \"show_images(\\\"Landfill\\\", images_to_show=3)\\n\",\n",
     "    \"show_images(\\\"Recycling\\\", images_to_show=3)\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"DEST_BUCKET = 's3://'+BUCKET+'/'+PREFIX+'/images/'\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"!aws s3 cp --recursive data/images $DEST_BUCKET --quiet\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"Ensure that the newly created directories containing the downloaded data are structured as shown at the beginning of this tutorial.\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"!aws s3 ls $DEST_BUCKET\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"#### Prepare \\\"list\\\" files with train-val split\\n\",\n",
     "    \"\\n\",\n",
     "    \"The image classification algorithm can take two types of input formats. The first is a [RecordIO format](https://mxnet.apache.org/api/faq/recordio) (content type: application/x-recordio) and the other is a Image list format (.lst file). These file formats allows for efficient loading of images when training the model. In this example we will be using the Image list format (.lst file). A .lst file is a tab-separated file with three columns that contains a list of image files. The first column specifies the image index, the second column specifies the class label index for the image, and the third column specifies the relative path of the image file. The RecordIO file contains the actual pixel data for the images.\\n\",\n",
     "    \"\\n\",\n",
     "    \"To be able to create the .rec files, we first need to split the data into training and validation sets (after shuffling) and create two list files for each. Here our split into train, validation and test (specified by the `0.7` parameter below for test). We keep 0.02% to test the model.\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"The image and lst files will be converted to RecordIO file internally by the image classification algorithm. But if you want do the conversion, the following cell shows how to do it using the [im2rec](https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py) tool. Note that this is just an example of creating RecordIO files. We are **_not_** using them for training in this notebook.\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"!python $mxnet_path/tools/im2rec.py --list --recursive --test-ratio=0.02 --train-ratio 0.7 trash data/images\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## Save lst files to S3\\n\",\n",
     "    \"Training models is easy with Amazon SageMaker. When youâ€™re ready to train in SageMaker, simply specify the location of your data in Amazon S3, and indicate the type and quantity of SageMaker ML instances you need. SageMaker sets up a distributed compute cluster, performs the training, outputs the result to Amazon S3, and tears down the cluster when complete. \\n\",\n",
     "    \"To use Amazon Sagemaker training we must first transfer our input data to Amazon S3.\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"s3train_lst = 's3://{}/{}/train_lst/'.format(BUCKET, PREFIX)\\n\",\n",
     "    \"s3validation_lst = 's3://{}/{}/validation_lst/'.format(BUCKET, PREFIX)\\n\",\n",
     "    \"\\n\",\n",
     "    \"# upload the lst files to train_lst and validation_lst channels\\n\",\n",
     "    \"!aws s3 cp trash_train.lst $s3train_lst --quiet\\n\",\n",
     "    \"!aws s3 cp trash_val.lst $s3validation_lst --quiet\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"#### Retrieve dataset size\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"Let's see the size of train, validation and test datasets\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"f = open('trash_train.lst', 'r')\\n\",\n",
     "    \"train_samples = sum(1 for line in f)\\n\",\n",
     "    \"f.close()\\n\",\n",
     "    \"f = open('trash_val.lst', 'r')\\n\",\n",
     "    \"val_samples = sum(1 for line in f)\\n\",\n",
     "    \"f.close()\\n\",\n",
     "    \"f = open('trash_test.lst', 'r')\\n\",\n",
     "    \"test_samples = sum(1 for line in f)\\n\",\n",
     "    \"f.close()\\n\",\n",
     "    \"print('train_samples:', train_samples)\\n\",\n",
     "    \"print('val_samples:', val_samples)\\n\",\n",
     "    \"print('test_samples:', test_samples)\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"This marks the end of the data preparation phase.\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"# Train the model\\n\",\n",
     "    \"\\n\",\n",
     "    \"Training a good model from scratch can take a long time. Fortunately, we're able to use transfer learning to fine-tune a model that has been trained on millions of images. Transfer learning allows us to train a model to recognize new classes in minutes instead of hours or days that it would normally take to train the model from scratch. Transfer learning requires a lot less data to train a model than from scratch (hundreds instead of tens of thousands).\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## Fine-tuning the Image Classification Model\\n\",\n",
     "    \"Now that we are done with all the setup that is needed, we are ready to train our trash detector. To begin, let us create a ``sageMaker.estimator.Estimator`` object. This estimator will launch the training job.\\n\",\n",
     "    \"### Training parameters\\n\",\n",
     "    \"There are two kinds of parameters that need to be set for training. The first one are the parameters for the training job. These include:\\n\",\n",
     "    \"\\n\",\n",
     "    \"* **Training instance count**: This is the number of instances on which to run the training. When the number of instances is greater than one, then the image classification algorithm will run in distributed settings. \\n\",\n",
     "    \"* **Training instance type**: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training \\n\",\n",
     "    \"* **Output path**: This the s3 folder in which the training output is stored\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"s3_output_location = 's3://{}/{}/output'.format(BUCKET, PREFIX)\\n\",\n",
     "    \"ic = sagemaker.estimator.Estimator(training_image,\\n\",\n",
     "    \"                                         role, \\n\",\n",
     "    \"                                         train_instance_count=1, \\n\",\n",
     "    \"                                         train_instance_type='ml.p2.xlarge',\\n\",\n",
     "    \"                                         train_volume_size = 50,\\n\",\n",
     "    \"                                         train_max_run = 360000,\\n\",\n",
     "    \"                                         input_mode= 'File',\\n\",\n",
     "    \"                                         output_path=s3_output_location,\\n\",\n",
     "    \"                                         sagemaker_session=sess,\\n\",\n",
     "    \"                                         base_job_name='ic-trash')\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"Apart from the above set of parameters, there are hyperparameters that are specific to the algorithm. These are:\\n\",\n",
     "    \"\\n\",\n",
     "    \"* **num_layers**: The number of layers (depth) for the network. We use 18 in this samples but other values such as 50, 152 can be used.\\n\",\n",
     "    \"* **use_pretrained_model**: Set to 1 to use pretrained model for transfer learning.\\n\",\n",
     "    \"* **image_shape**: The input image dimensions,'num_channels, height, width', for the network. It should be no larger than the actual image size. The number of channels should be same as the actual image.\\n\",\n",
     "    \"* **num_classes**: This is the number of output classes for the new dataset. For us, we have \\n\",\n",
     "    \"* **num_training_samples**: This is the total number of training samples. It is set to 15240 for caltech dataset with the current split.\\n\",\n",
     "    \"* **mini_batch_size**: The number of training samples used for each mini batch. In distributed training, the number of training samples used per batch will be N * mini_batch_size where N is the number of hosts on which training is run.\\n\",\n",
     "    \"* **epochs**: Number of training epochs.\\n\",\n",
     "    \"* **learning_rate**: Learning rate for training.\\n\",\n",
     "    \"* **top_k**: Report the top-k accuracy during training.\\n\",\n",
     "    \"* **resize**: Resize the image before using it for training. The images are resized so that the shortest side is of this parameter. If the parameter is not set, then the training data is used as such without resizing.\\n\",\n",
     "    \"* **precision_dtype**: Training datatype precision (default: float32). If set to 'float16', the training will be done in mixed_precision mode and will be faster than float32 mode\\n\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"ic.set_hyperparameters(num_layers=18,\\n\",\n",
     "    \"                             use_pretrained_model=1,\\n\",\n",
     "    \"                             image_shape = \\\"3,224,224\\\",\\n\",\n",
     "    \"                             num_classes=3,\\n\",\n",
     "    \"                             mini_batch_size=128,\\n\",\n",
     "    \"                             epochs=10,\\n\",\n",
     "    \"                             learning_rate=0.01,\\n\",\n",
     "    \"                             top_k=2,\\n\",\n",
     "    \"                             num_training_samples=train_samples,\\n\",\n",
     "    \"                             resize = 224,\\n\",\n",
     "    \"                             precision_dtype='float32')\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## Input data specification\\n\",\n",
     "    \"Set the data type and channels used for training\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"s3images = 's3://{}/{}/images/'.format(BUCKET, PREFIX)\\n\",\n",
     "    \"\\n\",\n",
     "    \"train_data = sagemaker.session.s3_input(s3images, distribution='FullyReplicated', \\n\",\n",
     "    \"                        content_type='application/x-image', s3_data_type='S3Prefix')\\n\",\n",
     "    \"validation_data = sagemaker.session.s3_input(s3images, distribution='FullyReplicated', \\n\",\n",
     "    \"                             content_type='application/x-image', s3_data_type='S3Prefix')\\n\",\n",
     "    \"train_data_lst = sagemaker.session.s3_input(s3train_lst, distribution='FullyReplicated', \\n\",\n",
     "    \"                        content_type='application/x-image', s3_data_type='S3Prefix')\\n\",\n",
     "    \"validation_data_lst = sagemaker.session.s3_input(s3validation_lst, distribution='FullyReplicated', \\n\",\n",
     "    \"                             content_type='application/x-image', s3_data_type='S3Prefix')\\n\",\n",
     "    \"\\n\",\n",
     "    \"data_channels = {'train': train_data, 'validation': validation_data, \\n\",\n",
     "    \"                 'train_lst': train_data_lst, 'validation_lst': validation_data_lst}\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## Start the training\\n\",\n",
     "    \"Start training by calling the fit method in the estimator\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"ic.fit(inputs=data_channels, logs=True)\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"#### The output from the above command will have the model accuracy and the time it took to run the training. \\n\",\n",
     "    \"#### You can also view these details by navigating to ``Training -> Training Jobs -> job_name -> View logs`` in the Amazon SageMaker console \"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"The model trained above can now be found in the `s3://<YOUR_BUCKET>/<PREFIX>/output` path.\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"MODEL_PATH = ic.model_data\\n\",\n",
     "    \"print(MODEL_PATH)\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## Deploy to a Sagemaker endpoint\\n\",\n",
     "    \"After training your model is complete, you can test your model by asking it to predict the class of a sample trash image that the model has not seen before. This step is called inference.\\n\",\n",
     "    \"\\n\",\n",
     "    \"Amazon SageMaker provides an HTTPS endpoint where your machine learning model is available to provide inferences. For more information see the [Amazon SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html).\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"ic_infer = ic.deploy(initial_instance_count=1, instance_type='local')\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## Test the images against the endpoint\\n\",\n",
     "    \"We will use the test images that were kept aside for testing.\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"object_categories = ['Compost', 'Landfill', 'Recycling']\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"from IPython.display import Image, display\\n\",\n",
     "    \"import json\\n\",\n",
     "    \"import numpy as np\\n\",\n",
     "    \"\\n\",\n",
     "    \"\\n\",\n",
     "    \"def test_model():\\n\",\n",
     "    \"    preds = []\\n\",\n",
     "    \"    acts  = []\\n\",\n",
     "    \"    num_errors = 0\\n\",\n",
     "    \"    with open('trash_test.lst', 'r') as f:\\n\",\n",
     "    \"        for line in f:\\n\",\n",
     "    \"            stripped_line = str(line.strip()).split(\\\"\\\\t\\\")\\n\",\n",
     "    \"            file_path = stripped_line[2]\\n\",\n",
     "    \"            category = int(float(stripped_line[1]))\\n\",\n",
     "    \"            with open(IMAGES_DIR + stripped_line[2], 'rb') as f:\\n\",\n",
     "    \"                payload = f.read()\\n\",\n",
     "    \"                payload = bytearray(payload)\\n\",\n",
     "    \"\\n\",\n",
     "    \"                ic_infer.content_type = 'application/x-image'\\n\",\n",
     "    \"                result = json.loads(ic_infer.predict(payload))\\n\",\n",
     "    \"            # the result will output the probabilities for all classes\\n\",\n",
     "    \"            # find the class with maximum probability and print the class index\\n\",\n",
     "    \"            index = np.argmax(result)\\n\",\n",
     "    \"            act = object_categories[category]\\n\",\n",
     "    \"            pred = object_categories[index]\\n\",\n",
     "    \"            conf = result[index]\\n\",\n",
     "    \"            print(\\\"Result: Predicted: {}, Confidence: {:.2f}, Actual: {} \\\".format(pred, conf, act))\\n\",\n",
     "    \"            acts.append(category)\\n\",\n",
     "    \"            preds.append(index)\\n\",\n",
     "    \"            if (pred != act):\\n\",\n",
     "    \"                num_errors += 1\\n\",\n",
     "    \"                print('ERROR on image -- Predicted: {}, Confidence: {:.2f}, Actual: {}'.format(pred, conf, act))\\n\",\n",
     "    \"            display(Image(filename=IMAGES_DIR + stripped_line[2], width=100, height=100))\\n\",\n",
     "    \"\\n\",\n",
     "    \"    return num_errors, preds, acts\\n\",\n",
     "    \"num_errors, preds, acts = test_model()\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"from sklearn.metrics import confusion_matrix\\n\",\n",
     "    \"import numpy as np\\n\",\n",
     "    \"import itertools\\n\",\n",
     "    \"COLOR = 'green'\\n\",\n",
     "    \"plt.rcParams['text.color'] = COLOR\\n\",\n",
     "    \"plt.rcParams['axes.labelcolor'] = COLOR\\n\",\n",
     "    \"plt.rcParams['xtick.color'] = COLOR\\n\",\n",
     "    \"plt.rcParams['ytick.color'] = COLOR\\n\",\n",
     "    \"def plot_confusion_matrix(cm, classes,\\n\",\n",
     "    \"                          class_name_list,\\n\",\n",
     "    \"                          normalize=False,\\n\",\n",
     "    \"                          title='Confusion matrix',\\n\",\n",
     "    \"                          cmap=plt.cm.GnBu):\\n\",\n",
     "    \"    plt.figure(figsize=(7,7))\\n\",\n",
     "    \"    plt.grid(False)\\n\",\n",
     "    \"\\n\",\n",
     "    \"    plt.imshow(cm, interpolation='nearest', cmap=cmap)\\n\",\n",
     "    \"    plt.title(title)\\n\",\n",
     "    \"    tick_marks = np.arange(len(classes))\\n\",\n",
     "    \"    plt.xticks(tick_marks, classes, rotation=45)\\n\",\n",
     "    \"    plt.yticks(tick_marks, classes)\\n\",\n",
     "    \"    fmt = '.2f' if normalize else 'd'\\n\",\n",
     "    \"    thresh = cm.max() / 2.\\n\",\n",
     "    \"    for i, j in itertools.product(range(cm.shape[0]), \\n\",\n",
     "    \"                                  range(cm.shape[1])):\\n\",\n",
     "    \"        plt.text(j, i, format(cm[i, j], fmt),\\n\",\n",
     "    \"                 horizontalalignment=\\\"center\\\",\\n\",\n",
     "    \"                 color=\\\"white\\\" if cm[i, j] > thresh else \\\"black\\\")\\n\",\n",
     "    \"    plt.tight_layout()\\n\",\n",
     "    \"    plt.gca().set_xticklabels(class_name_list)\\n\",\n",
     "    \"    plt.gca().set_yticklabels(class_name_list)\\n\",\n",
     "    \"    plt.ylabel('True label')\\n\",\n",
     "    \"    plt.xlabel('Predicted label')\\n\",\n",
     "    \"\\n\",\n",
     "    \"def create_and_plot_confusion_matrix(actual, predicted):\\n\",\n",
     "    \"    cnf_matrix = confusion_matrix(actual, np.asarray(predicted),labels=range(len(object_categories)))\\n\",\n",
     "    \"    plot_confusion_matrix(cnf_matrix, classes=range(len(object_categories)), class_name_list=object_categories)\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## Display confusion matrix showing 'true' and 'predicted' labels\\n\",\n",
     "    \"\\n\",\n",
     "    \"A confusion matrix is a table that is often used to describe the performance of a classification model (or \\\"classifier\\\") on a set of test data for which the true values are known. It's a table  with two dimensions (\\\"actual\\\" and \\\"predicted\\\"), and identical sets of \\\"classes\\\" in both dimensions (each combination of dimension and class is a variable in the contingency table). The diagonal values in the table indicate a match between the predicted class and the actual class. \\n\",\n",
     "    \"\\n\",\n",
     "    \"For more details go to [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) (Wikipedia)\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"create_and_plot_confusion_matrix(acts, preds)\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## Approximate costs\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"As of 03/11/2020 and based on the pricing information displayed on the page: https://aws.amazon.com/sagemaker/pricing/, here's the costs you can expect in a 24 hour period:\\n\",\n",
     "    \"\\n\",\n",
     "    \" - Notebook instance cost **\\\\\\\\$6** Assuming you choose ml.t3.xlarge (\\\\\\\\$0.233/hour) instance. This can vary based on the size of instance you choose.\\n\",\n",
     "    \" - Training costs **\\\\\\\\$1.05** : Assuming you will run about 10 training runs in a 24 hour period using the sample dataset provided. The notebook uses a p2.xlarge (\\\\\\\\$1.26/hour) instance\\n\",\n",
     "    \" - Model hosting **\\\\\\\\$6.72** : Assuming you use the ml.m4.xlarge (\\\\\\\\$0.28/hour) instance running for 24 hours. \\n\",\n",
     "    \" \\n\",\n",
     "    \"*NOTE* : To save on costs, stop your notebook instances and delete the model edpoint when not in use\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## (Optional) Clean-up\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"If you're ready to be done with this notebook, please run the cell below. This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on.\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"sess.delete_endpoint(ic_infer.endpoint)\\n\",\n",
     "    \"print(\\\"Completed\\\")\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## Rename model to deploy to AWS DeepLens\\n\",\n",
     "    \"The MxNet model that is stored in the S3 bucket contains 2 files: the params file and a symbol.json file. To simplify deployment to AWS DeepLens, we'll modify the params file so that you do not need to specify the number of epochs the model was trained for.\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"import glob\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"!rm -rf data/$PREFIX/tmp && mkdir -p data/$PREFIX/tmp\\n\",\n",
     "    \"!aws s3 cp $MODEL_PATH data/$PREFIX/tmp\\n\",\n",
     "    \"!tar -xzvf data/$PREFIX/tmp/model.tar.gz -C data/$PREFIX/tmp\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"params_file_name = glob.glob('./data/' + PREFIX + '/tmp/*.params')[0]\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"!mv $params_file_name data/$PREFIX/tmp/image-classification-0000.params\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"!tar -cvzf ./model.tar.gz -C data/$PREFIX/tmp ./image-classification-0000.params ./image-classification-symbol.json\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": [\n",
     "    \"!aws s3 cp model.tar.gz $MODEL_PATH\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"markdown\",\n",
     "   \"metadata\": {},\n",
     "   \"source\": [\n",
     "    \"## Next steps\\n\",\n",
     "    \"\\n\",\n",
     "    \"At this point, you have completed:\\n\",\n",
     "    \"* Training a model with Amazon Sagemaker using transfer learning\\n\",\n",
     "    \"\\n\",\n",
     "    \"Next you'll deploy this model to AWS DeepLens. If you have started this notebook as part of a tutorial, please go back to the next step in the tutorial. If you have found this notebook through other channels, please go to [awsdeeplens.recipes](http://awsdeeplens.recipes) and select the Trash Detector tutorial to continue.\"\n",
     "   ]\n",
     "  },\n",
     "  {\n",
     "   \"cell_type\": \"code\",\n",
     "   \"execution_count\": null,\n",
     "   \"metadata\": {},\n",
     "   \"outputs\": [],\n",
     "   \"source\": []\n",
     "  }\n",
     " ],\n",
     " \"metadata\": {\n",
     "  \"kernelspec\": {\n",
     "   \"display_name\": \"conda_mxnet_p36\",\n",
     "   \"language\": \"python\",\n",
     "   \"name\": \"conda_mxnet_p36\"\n",
     "  },\n",
     "  \"language_info\": {\n",
     "   \"codemirror_mode\": {\n",
     "    \"name\": \"ipython\",\n",
     "    \"version\": 3\n",
     "   },\n",
     "   \"file_extension\": \".py\",\n",
     "   \"mimetype\": \"text/x-python\",\n",
     "   \"name\": \"python\",\n",
     "   \"nbconvert_exporter\": \"python\",\n",
     "   \"pygments_lexer\": \"ipython3\",\n",
     "   \"version\": \"3.6.5\"\n",
     "  }\n",
     " },\n",
     " \"nbformat\": 4,\n",
     " \"nbformat_minor\": 4\n",
     "}"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}